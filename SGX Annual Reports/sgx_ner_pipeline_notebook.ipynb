{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa6759f",
   "metadata": {},
   "source": [
    "# SGX Annual Report NER Pipeline (Notebook Format)\n",
    "\n",
    "> **Goal**: Extract **people**, **organizations**, and **industry sectors** from SGX annual‑report PDFs, then infer relationships (person ↔ org, org ↔ industry) and export everything to CSV — without using LLMs.\n",
    ">\n",
    "> This notebook reorganises the original `ner.py` script into clearly separated, runnable sections.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Imports & Global Config](#2-imports--global-config)\n",
    "3. [PDF Utilities](#3-pdf-utilities)\n",
    "4. [Entity Extraction](#4-entity-extraction)\n",
    "5. [Relationship Inference](#5-relationship-inference)\n",
    "6. [Batch Processing Helpers](#6-batch-processing-helpers)\n",
    "7. [Run the Pipeline](#7-run-the-pipeline)\n",
    "8. [Combine Outputs](#8-combine-outputs)\n",
    "9. [Next Steps / TODOs](#9-next-steps--todos)\n",
    "\n",
    "---\n",
    "\n",
    "## 1  Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for blis (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [36 lines of output]\n",
      "      BLIS_COMPILER? None\n",
      "      C:\\Users\\22601\\AppData\\Local\\Temp\\pip-build-env-kvwterci\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: BSD License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\about.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\benchmark.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      creating build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\tests\\common.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\tests\\test_dotv.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\tests\\test_gemm.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\cy.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\py.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\cy.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\__init__.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      running build_ext\n",
      "      Build options win32 msvc\n",
      "      BUILD ARCH: x86_64\n",
      "      {'ACLOCAL_PATH': 'C:\\\\Program Files\\\\Git\\\\mingw64\\\\share\\\\aclocal;C:\\\\Program Files\\\\Git\\\\usr\\\\share\\\\aclocal', 'ACTIONS_RUNNER_ACTION_ARCHIVE_CACHE': 'C:\\\\actionarchivecache\\\\', 'AGENT_BUILDDIRECTORY': 'D:\\\\a\\\\1', 'AGENT_CLOUDID': 'cc0b709a-2cb2-4727-8ec5-adb402963aba', 'AGENT_DISABLELOGPLUGIN_TESTFILEPUBLISHERPLUGIN': 'true', 'AGENT_DISABLELOGPLUGIN_TESTRESULTLOGPLUGIN': 'false', 'AGENT_ENABLE_PIPELINEARTIFACT_LARGE_CHUNK_SIZE': 'true', 'AGENT_HOMEDIRECTORY': 'C:\\\\agents\\\\3.248.0', 'AGENT_ID': '93', 'AGENT_ISSELFHOSTED': '0', 'AGENT_JOBNAME': 'JSONL Python39Windows', 'AGENT_JOBSTATUS': 'Succeeded', 'AGENT_LOGTOBLOBSTORAGESERVICE': 'true', 'AGENT_MACHINENAME': 'fv-az618-395', 'AGENT_NAME': 'Azure Pipelines 2', 'AGENT_OS': 'Windows_NT', 'AGENT_OSARCHITECTURE': 'X64', 'AGENT_READONLYVARIABLES': 'true', 'AGENT_RETAINDEFAULTENCODING': 'false', 'AGENT_ROOTDIRECTORY': 'D:\\\\a', 'AGENT_SERVEROMDIRECTORY': 'C:\\\\agents\\\\3.248.0\\\\externals\\\\vstsom', 'AGENT_TASKRESTRICTIONSENFORCEMENTMODE': 'Enabled', 'AGENT_TEMPDIRECTORY': 'D:\\\\a\\\\_temp', 'AGENT_TOOLSDIRECTORY': 'C:\\\\hostedtoolcache\\\\windows', 'AGENT_USEWORKSPACEID': 'true', 'AGENT_USE_FETCH_FILTER_IN_CHECKOUT_TASK': 'true', 'AGENT_VERSION': '4.248.0', 'AGENT_WORKFOLDER': 'D:\\\\a', 'ALLUSERSPROFILE': 'C:\\\\ProgramData', 'ANDROID_HOME': 'C:\\\\Android\\\\android-sdk', 'ANDROID_NDK': 'C:\\\\Android\\\\android-sdk\\\\ndk\\\\27.2.12479018', 'ANDROID_NDK_HOME': 'C:\\\\Android\\\\android-sdk\\\\ndk\\\\27.2.12479018', 'ANDROID_NDK_LATEST_HOME': 'C:\\\\Android\\\\android-sdk\\\\ndk\\\\27.2.12479018', 'ANDROID_NDK_ROOT': 'C:\\\\Android\\\\android-sdk\\\\ndk\\\\27.2.12479018', 'ANDROID_SDK_ROOT': 'C:\\\\Android\\\\android-sdk', 'ANT_HOME': 'C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\ant\\\\tools\\\\apache-ant-1.10.14', 'APPDATA': 'C:\\\\Users\\\\VssAdministrator\\\\AppData\\\\Roaming', 'AZP_75787_ENABLE_COLLECT': 'true', 'AZP_75787_ENABLE_NEW_LOGIC': 'false', 'AZP_75787_ENABLE_NEW_LOGIC_LOG': 'false', 'AZP_75787_ENABLE_NEW_PH_LOGIC': 'true', 'AZP_AGENT_CHECK_FOR_TASK_DEPRECATION': 'true', 'AZP_AGENT_IGNORE_VSTSTASKLIB': 'true', 'AZP_AGENT_LOG_TASKNAME_IN_USERAGENT': 'true', 'AZP_AGENT_MOUNT_WORKSPACE': 'true', 'AZP_ENABLE_RESOURCE_MONITOR_DEBUG_OUTPUT': 'true', 'AZP_ENABLE_RESOURCE_UTILIZATION_WARNINGS': 'true', 'AZP_PS_ENABLE_INVOKE_PROCESS': 'true', 'AZURE_CONFIG_DIR': 'C:\\\\azureCli', 'AZURE_DEVOPS_CACHE_DIR': 'C:\\\\azureDevOpsCli\\\\cache', 'AZURE_EXTENSION_DIR': 'C:\\\\Program Files\\\\Common Files\\\\AzureCliExtensionDirectory', 'AZURE_HTTP_USER_AGENT': 'VSTS_116cc368-5c0c-4eb4-bb44-7f3fa5bdce14_build_6_0', 'AZ_DEVOPS_GLOBAL_CONFIG_DIR': 'C:\\\\azureDevOpsCli', 'BUILD_ARTIFACTSTAGINGDIRECTORY': 'D:\\\\a\\\\1\\\\a', 'BUILD_BINARIESDIRECTORY': 'D:\\\\a\\\\1\\\\b', 'BUILD_BUILDID': '27184', 'BUILD_BUILDNUMBER': '20250110.10', 'BUILD_BUILDURI': 'vstfs:///Build/Build/27184', 'BUILD_CONTAINERID': '29126447', 'BUILD_DEFINITIONFOLDERPATH': '\\\\', 'BUILD_DEFINITIONNAME': 'explosion.cython-blis', 'BUILD_DEFINITIONVERSION': '1', 'BUILD_QUEUEDBY': 'Microsoft.VisualStudio.Services.TFS', 'BUILD_QUEUEDBYID': '00000002-0000-8888-8000-000000000000', 'BUILD_REASON': 'BatchedCI', 'BUILD_REPOSITORY_CLEAN': 'False', 'BUILD_REPOSITORY_GIT_SUBMODULECHECKOUT': 'False', 'BUILD_REPOSITORY_ID': 'explosion/cython-blis', 'BUILD_REPOSITORY_LOCALPATH': 'D:\\\\a\\\\1\\\\s', 'BUILD_REPOSITORY_NAME': 'explosion/cython-blis', 'BUILD_REPOSITORY_PROVIDER': 'GitHub', 'BUILD_REPOSITORY_URI': 'https://github.com/explosion/cython-blis', 'BUILD_REQUESTEDFOR': 'Microsoft.VisualStudio.Services.TFS', 'BUILD_REQUESTEDFOREMAIL': '', 'BUILD_REQUESTEDFORID': '00000002-0000-8888-8000-000000000000', 'BUILD_SOURCEBRANCH': 'refs/heads/v0.8.x', 'BUILD_SOURCEBRANCHNAME': 'v0.8.x', 'BUILD_SOURCESDIRECTORY': 'D:\\\\a\\\\1\\\\s', 'BUILD_SOURCEVERSION': '2cb93566856d208bc951d5847171583d6379406a', 'BUILD_SOURCEVERSIONAUTHOR': 'honnibal', 'BUILD_SOURCEVERSIONMESSAGE': 'Edit ap', 'BUILD_STAGINGDIRECTORY': 'D:\\\\a\\\\1\\\\a', 'CABAL_DIR': 'C:\\\\cabal', 'COBERTURA_HOME': 'C:\\\\cobertura-2.1.1', 'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files', 'COMMON_TESTRESULTSDIRECTORY': 'D:\\\\a\\\\1\\\\TestResults', 'COMPUTERNAME': 'fv-az618-395', 'COMSPEC': 'C:\\\\Windows\\\\system32\\\\cmd.exe', 'CONDA': 'C:\\\\Miniconda', 'CONFIG_SITE': 'C:/Program Files/Git/etc/config.site', 'COPYFILESOVERSSHV0_USE_QUEUE': 'true', 'CHOCOLATEYINSTALL': 'C:\\\\ProgramData\\\\chocolatey', 'CHROMEWEBDRIVER': 'C:\\\\SeleniumWebDrivers\\\\ChromeDriver', 'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files', 'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files', 'DISPLAY': 'needs-to-be-defined', 'DISTRIBUTEDTASK_AGENT_ADDFORCECREDENTIALSTOGITCHECKOUT': 'True', 'DISTRIBUTEDTASK_AGENT_AGENTENABLEPIPELINEARTIFACTLARGECHUNKSIZE': 'True', 'DISTRIBUTEDTASK_AGENT_CHECKIFTASKNODERUNNERISDEPRECATED246': 'True', 'DISTRIBUTEDTASK_AGENT_CONTINUEAFTERCANCELPROCESSTREEKILLATTEMPT': 'True', 'DISTRIBUTEDTASK_AGENT_DOCKERACTIONRETRIES': 'True', 'DISTRIBUTEDTASK_AGENT_ENABLEADDITIONALMASKINGREGEXES': 'True', 'DISTRIBUTEDTASK_AGENT_ENABLEISSUESOURCEVALIDATION': 'True', 'DISTRIBUTEDTASK_AGENT_ENABLERESOURCEMONITORDEBUGOUTPUT': 'True', 'DISTRIBUTEDTASK_AGENT_ENABLERESOURCEUTILIZATIONWARNINGS': 'True', 'DISTRIBUTEDTASK_AGENT_FAILDEPRECATEDBUILDTASK': 'True', 'DISTRIBUTEDTASK_AGENT_FAILDEPRECATEDTASK': 'True', 'DISTRIBUTEDTASK_AGENT_FAILJOBWHENAGENTDIES': 'True', 'DISTRIBUTEDTASK_AGENT_FIXPOSSIBLEGITOUTOFMEMORYPROBLEM': 'False', 'DISTRIBUTEDTASK_AGENT_FORCEUPDATETOLATEST2VERSION': 'False', 'DISTRIBUTEDTASK_AGENT_IGNOREVSTSTASKLIB': 'True', 'DISTRIBUTEDTASK_AGENT_LOGTASKNAMEINUSERAGENT': 'True', 'DISTRIBUTEDTASK_AGENT_LOGTOBLOBSTORAGESERVICE': 'True', 'DISTRIBUTEDTASK_AGENT_MOUNTWORKSPACE': 'True', 'DISTRIBUTEDTASK_AGENT_READONLYVARIABLES': 'True', 'DISTRIBUTEDTASK_AGENT_ROSETTA2WARNING': 'True', 'DISTRIBUTEDTASK_AGENT_USEDOCKERCOMPOSEV2COMPATIBLEMODE': 'False', 'DISTRIBUTEDTASK_AGENT_USEFETCHFILTERINCHECKOUTTASK': 'True', 'DISTRIBUTEDTASK_AGENT_USEGITLONGPATHS': 'True', 'DISTRIBUTEDTASK_AGENT_USELATESTGITVERSION': 'True', 'DISTRIBUTEDTASK_AGENT_USEMSALLIBRARY': 'True', 'DISTRIBUTEDTASK_AGENT_USENEWNODEHANDLERTELEMETRY': 'True', 'DISTRIBUTEDTASK_AGENT_USENODE20TOSTARTCONTAINER': 'True', 'DISTRIBUTEDTASK_AGENT_USEWORKSPACEID': 'True', 'DISTRIBUTEDTASK_TASKS_COPYFILESOVERSSHV0USEQUEUE': 'True', 'DISTRIBUTEDTASK_TASKS_NODE_SKIPDEBUGLOGSWHENDEBUGMODEOFF': 'True', 'DISTRIBUTEDTASK_TASKS_RETIREAZURERMPOWERSHELLMODULE': 'True', 'DOTNET_MULTILEVEL_LOOKUP': '0', 'DOTNET_NOLOGO': '1', 'DOTNET_SKIP_FIRST_TIME_EXPERIENCE': '1', 'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData', 'ENABLE_ISSUE_SOURCE_VALIDATION': 'true', 'EXEPATH': 'C:\\\\Program Files\\\\Git\\\\bin', 'EDGEWEBDRIVER': 'C:\\\\SeleniumWebDrivers\\\\EdgeDriver', 'FAIL_DEPRECATED_BUILD_TASK': 'true', 'FAIL_DEPRECATED_TASK': 'true', 'FAIL_JOB_WHEN_AGENT_DIES': 'true', 'GCM_INTERACTIVE': 'Never', 'GHCUP_INSTALL_BASE_PREFIX': 'C:\\\\', 'GHCUP_MSYS2': 'C:\\\\msys64', 'GIT_TERMINAL_PROMPT': '0', 'GOROOT_1_20_X64': 'C:\\\\hostedtoolcache\\\\windows\\\\go\\\\1.20.14\\\\x64', 'GOROOT_1_21_X64': 'C:\\\\hostedtoolcache\\\\windows\\\\go\\\\1.21.13\\\\x64', 'GOROOT_1_22_X64': 'C:\\\\hostedtoolcache\\\\windows\\\\go\\\\1.22.10\\\\x64', 'GOROOT_1_23_X64': 'C:\\\\hostedtoolcache\\\\windows\\\\go\\\\1.23.4\\\\x64', 'GRADLE_HOME': 'C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\gradle\\\\tools\\\\gradle-8.12', 'GECKOWEBDRIVER': 'C:\\\\SeleniumWebDrivers\\\\GeckoDriver', 'HOME': 'C:\\\\Users\\\\VssAdministrator', 'HOMEDRIVE': 'C:', 'HOMEPATH': '\\\\Users\\\\VssAdministrator', 'HOSTNAME': 'fv-az618-395', 'IEWEBDRIVER': 'C:\\\\SeleniumWebDrivers\\\\IEDriver', 'IMAGENAME': 'windows-latest', 'INFOPATH': 'C:\\\\Program Files\\\\Git\\\\mingw64\\\\local\\\\info;C:\\\\Program Files\\\\Git\\\\mingw64\\\\share\\\\info;C:\\\\Program Files\\\\Git\\\\usr\\\\local\\\\info;C:\\\\Program Files\\\\Git\\\\usr\\\\share\\\\info;C:\\\\Program Files\\\\Git\\\\usr\\\\info;C:\\\\Program Files\\\\Git\\\\share\\\\info', 'IMAGEOS': 'win22', 'IMAGEVERSION': '20250105.1.0', 'JAVA_HOME': 'C:\\\\hostedtoolcache\\\\windows\\\\Java_Temurin-Hotspot_jdk\\\\8.0.432-6\\\\x64', 'JAVA_HOME_11_X64': 'C:\\\\hostedtoolcache\\\\windows\\\\Java_Temurin-Hotspot_jdk\\\\11.0.25-9\\\\x64', 'JAVA_HOME_17_X64': 'C:\\\\hostedtoolcache\\\\windows\\\\Java_Temurin-Hotspot_jdk\\\\17.0.13-11\\\\x64', 'JAVA_HOME_21_X64': 'C:\\\\hostedtoolcache\\\\windows\\\\Java_Temurin-Hotspot_jdk\\\\21.0.5-11.0\\\\x64', 'JAVA_HOME_8_X64': 'C:\\\\hostedtoolcache\\\\windows\\\\Java_Temurin-Hotspot_jdk\\\\8.0.432-6\\\\x64', 'LANG': 'en_US.UTF-8', 'LOCALAPPDATA': 'C:\\\\Users\\\\VssAdministrator\\\\AppData\\\\Local', 'LOGONSERVER': '\\\\\\\\fv-az618-395', 'M2': 'C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\maven\\\\apache-maven-3.8.7\\\\bin', 'M2_REPO': 'C:\\\\ProgramData\\\\m2', 'MANPATH': 'C:\\\\Program Files\\\\Git\\\\mingw64\\\\local\\\\man;C:\\\\Program Files\\\\Git\\\\mingw64\\\\share\\\\man;C:\\\\Program Files\\\\Git\\\\usr\\\\local\\\\man;C:\\\\Program Files\\\\Git\\\\usr\\\\share\\\\man;C:\\\\Program Files\\\\Git\\\\usr\\\\man;C:\\\\Program Files\\\\Git\\\\share\\\\man', 'MAVEN_OPTS': '-Xms256m', 'MINGW_CHOST': 'x86_64-w64-mingw32', 'MINGW_PACKAGE_PREFIX': 'mingw-w64-x86_64', 'MINGW_PREFIX': 'C:/Program Files/Git/mingw64', 'MSDEPLOY_HTTP_USER_AGENT': 'VSTS_116cc368-5c0c-4eb4-bb44-7f3fa5bdce14_build_6_0', 'MSYSTEM': 'MINGW64', 'MSYSTEM_CARCH': 'x86_64', 'MSYSTEM_CHOST': 'x86_64-w64-mingw32', 'MSYSTEM_PREFIX': 'C:/Program Files/Git/mingw64', 'NUMBER_OF_PROCESSORS': '2', 'OLDPWD': 'D:/a/1/s', 'ORIGINAL_PATH': 'C:\\\\Program Files\\\\Git\\\\mingw64\\\\bin;C:\\\\Program Files\\\\Git\\\\usr\\\\bin;C:\\\\Users\\\\VssAdministrator\\\\bin;C:\\\\Users\\\\VssAdministrator\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts;C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64\\\\Scripts;C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64;C:\\\\agents\\\\3.248.0\\\\externals\\\\git\\\\cmd;C:\\\\agents\\\\3.248.0\\\\externals\\\\git\\\\mingw64\\\\bin;C:\\\\Program Files\\\\MongoDB\\\\Server\\\\5.0\\\\bin;C:\\\\aliyun-cli;C:\\\\vcpkg;C:\\\\Program Files (x86)\\\\NSIS;C:\\\\tools\\\\zstd;C:\\\\Program Files\\\\Mercurial;C:\\\\hostedtoolcache\\\\windows\\\\stack\\\\3.3.1\\\\x64;C:\\\\cabal\\\\bin;C:\\\\ghcup\\\\bin;C:\\\\mingw64\\\\bin;C:\\\\Program Files\\\\dotnet;C:\\\\Program Files\\\\MySQL\\\\MySQL Server 8.0\\\\bin;C:\\\\Program Files\\\\R\\\\R-4.4.2\\\\bin\\\\x64;C:\\\\SeleniumWebDrivers\\\\GeckoDriver;C:\\\\SeleniumWebDrivers\\\\EdgeDriver;C:\\\\SeleniumWebDrivers\\\\ChromeDriver;C:\\\\Program Files (x86)\\\\sbt\\\\bin;C:\\\\Program Files (x86)\\\\GitHub CLI;C:\\\\Program Files\\\\Git\\\\usr\\\\bin;C:\\\\Program Files (x86)\\\\pipx_bin;C:\\\\npm\\\\prefix;C:\\\\hostedtoolcache\\\\windows\\\\go\\\\1.21.13\\\\x64\\\\bin;C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64\\\\Scripts;C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64;C:\\\\hostedtoolcache\\\\windows\\\\Ruby\\\\3.0.7\\\\x64\\\\bin;C:\\\\Program Files\\\\OpenSSL\\\\bin;C:\\\\tools\\\\kotlinc\\\\bin;C:\\\\hostedtoolcache\\\\windows\\\\Java_Temurin-Hotspot_jdk\\\\8.0.432-6\\\\x64\\\\bin;C:\\\\Program Files\\\\ImageMagick-7.1.1-Q16-HDRI;C:\\\\Program Files\\\\Microsoft SDKs\\\\Azure\\\\CLI2\\\\wbin;C:\\\\ProgramData\\\\kind;C:\\\\ProgramData\\\\Chocolatey\\\\bin;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\Windows\\\\System32\\\\OpenSSH;C:\\\\Program Files\\\\dotnet;C:\\\\Program Files\\\\PowerShell\\\\7;C:\\\\Program Files\\\\Microsoft\\\\Web Platform Installer;C:\\\\Program Files\\\\TortoiseSVN\\\\bin;C:\\\\Program Files\\\\Microsoft SQL Server\\\\Client SDK\\\\ODBC\\\\170\\\\Tools\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL Server\\\\150\\\\Tools\\\\Binn;C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Windows Performance Toolkit;C:\\\\Program Files (x86)\\\\WiX Toolset v3.14\\\\bin;C:\\\\Program Files\\\\Microsoft SQL Server\\\\130\\\\DTS\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL Server\\\\140\\\\DTS\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL Server\\\\150\\\\DTS\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL Server\\\\160\\\\DTS\\\\Binn;C:\\\\Strawberry\\\\c\\\\bin;C:\\\\Strawberry\\\\perl\\\\site\\\\bin;C:\\\\Strawberry\\\\perl\\\\bin;C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\pulumi\\\\tools\\\\Pulumi\\\\bin;C:\\\\Program Files\\\\CMake\\\\bin;C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\maven\\\\apache-maven-3.8.7\\\\bin;C:\\\\Program Files\\\\Microsoft Service Fabric\\\\bin\\\\Fabric\\\\Fabric.Code;C:\\\\Program Files\\\\Microsoft SDKs\\\\Service Fabric\\\\Tools\\\\ServiceFabricLocalClusterManager;C:\\\\Program Files\\\\nodejs;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Git\\\\mingw64\\\\bin;C:\\\\Program Files\\\\Git\\\\usr\\\\bin;C:\\\\Program Files\\\\GitHub CLI;C:\\\\tools\\\\php;C:\\\\Program Files (x86)\\\\sbt\\\\bin;C:\\\\Program Files\\\\Amazon\\\\AWSCLIV2;C:\\\\Program Files\\\\Amazon\\\\SessionManagerPlugin\\\\bin;C:\\\\Program Files\\\\Amazon\\\\AWSSAMCLI\\\\bin;C:\\\\Program Files\\\\Microsoft SQL Server\\\\130\\\\Tools\\\\Binn;C:\\\\Program Files\\\\LLVM\\\\bin;C:\\\\Users\\\\VssAdministrator\\\\.dotnet\\\\tools;C:\\\\Users\\\\VssAdministrator\\\\.cargo\\\\bin;C:\\\\Users\\\\VssAdministrator\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps', 'ORIGINAL_TEMP': 'C:/Users/VSSADM~1/AppData/Local/Temp', 'ORIGINAL_TMP': 'C:/Users/VSSADM~1/AppData/Local/Temp', 'OS': 'windows', 'PATH': 'C:\\\\Users\\\\VssAdministrator\\\\bin;C:\\\\Program Files\\\\Git\\\\mingw64\\\\bin;C:\\\\Program Files\\\\Git\\\\usr\\\\local\\\\bin;C:\\\\Program Files\\\\Git\\\\usr\\\\bin;C:\\\\Program Files\\\\Git\\\\usr\\\\bin;C:\\\\Program Files\\\\Git\\\\mingw64\\\\bin;C:\\\\Program Files\\\\Git\\\\usr\\\\bin;C:\\\\Users\\\\VssAdministrator\\\\bin;C:\\\\Users\\\\VssAdministrator\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts;C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64\\\\Scripts;C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64;C:\\\\agents\\\\3.248.0\\\\externals\\\\git\\\\cmd;C:\\\\agents\\\\3.248.0\\\\externals\\\\git\\\\mingw64\\\\bin;C:\\\\Program Files\\\\MongoDB\\\\Server\\\\5.0\\\\bin;C:\\\\aliyun-cli;C:\\\\vcpkg;C:\\\\Program Files (x86)\\\\NSIS;C:\\\\tools\\\\zstd;C:\\\\Program Files\\\\Mercurial;C:\\\\hostedtoolcache\\\\windows\\\\stack\\\\3.3.1\\\\x64;C:\\\\cabal\\\\bin;C:\\\\ghcup\\\\bin;C:\\\\mingw64\\\\bin;C:\\\\Program Files\\\\dotnet;C:\\\\Program Files\\\\MySQL\\\\MySQL Server 8.0\\\\bin;C:\\\\Program Files\\\\R\\\\R-4.4.2\\\\bin\\\\x64;C:\\\\SeleniumWebDrivers\\\\GeckoDriver;C:\\\\SeleniumWebDrivers\\\\EdgeDriver;C:\\\\SeleniumWebDrivers\\\\ChromeDriver;C:\\\\Program Files (x86)\\\\sbt\\\\bin;C:\\\\Program Files (x86)\\\\GitHub CLI;C:\\\\Program Files\\\\Git\\\\usr\\\\bin;C:\\\\Program Files (x86)\\\\pipx_bin;C:\\\\npm\\\\prefix;C:\\\\hostedtoolcache\\\\windows\\\\go\\\\1.21.13\\\\x64\\\\bin;C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64\\\\Scripts;C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64;C:\\\\hostedtoolcache\\\\windows\\\\Ruby\\\\3.0.7\\\\x64\\\\bin;C:\\\\Program Files\\\\OpenSSL\\\\bin;C:\\\\tools\\\\kotlinc\\\\bin;C:\\\\hostedtoolcache\\\\windows\\\\Java_Temurin-Hotspot_jdk\\\\8.0.432-6\\\\x64\\\\bin;C:\\\\Program Files\\\\ImageMagick-7.1.1-Q16-HDRI;C:\\\\Program Files\\\\Microsoft SDKs\\\\Azure\\\\CLI2\\\\wbin;C:\\\\ProgramData\\\\kind;C:\\\\ProgramData\\\\Chocolatey\\\\bin;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\Windows\\\\System32\\\\OpenSSH;C:\\\\Program Files\\\\dotnet;C:\\\\Program Files\\\\PowerShell\\\\7;C:\\\\Program Files\\\\Microsoft\\\\Web Platform Installer;C:\\\\Program Files\\\\TortoiseSVN\\\\bin;C:\\\\Program Files\\\\Microsoft SQL Server\\\\Client SDK\\\\ODBC\\\\170\\\\Tools\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL Server\\\\150\\\\Tools\\\\Binn;C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Windows Performance Toolkit;C:\\\\Program Files (x86)\\\\WiX Toolset v3.14\\\\bin;C:\\\\Program Files\\\\Microsoft SQL Server\\\\130\\\\DTS\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL Server\\\\140\\\\DTS\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL Server\\\\150\\\\DTS\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL Server\\\\160\\\\DTS\\\\Binn;C:\\\\Strawberry\\\\c\\\\bin;C:\\\\Strawberry\\\\perl\\\\site\\\\bin;C:\\\\Strawberry\\\\perl\\\\bin;C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\pulumi\\\\tools\\\\Pulumi\\\\bin;C:\\\\Program Files\\\\CMake\\\\bin;C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\maven\\\\apache-maven-3.8.7\\\\bin;C:\\\\Program Files\\\\Microsoft Service Fabric\\\\bin\\\\Fabric\\\\Fabric.Code;C:\\\\Program Files\\\\Microsoft SDKs\\\\Service Fabric\\\\Tools\\\\ServiceFabricLocalClusterManager;C:\\\\Program Files\\\\nodejs;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Git\\\\mingw64\\\\bin;C:\\\\Program Files\\\\Git\\\\usr\\\\bin;C:\\\\Program Files\\\\GitHub CLI;C:\\\\tools\\\\php;C:\\\\Program Files (x86)\\\\sbt\\\\bin;C:\\\\Program Files\\\\Amazon\\\\AWSCLIV2;C:\\\\Program Files\\\\Amazon\\\\SessionManagerPlugin\\\\bin;C:\\\\Program Files\\\\Amazon\\\\AWSSAMCLI\\\\bin;C:\\\\Program Files\\\\Microsoft SQL Server\\\\130\\\\Tools\\\\Binn;C:\\\\Program Files\\\\LLVM\\\\bin;C:\\\\Users\\\\VssAdministrator\\\\.dotnet\\\\tools;C:\\\\Users\\\\VssAdministrator\\\\.cargo\\\\bin;C:\\\\Users\\\\VssAdministrator\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Program Files\\\\Git\\\\usr\\\\bin\\\\vendor_perl;C:\\\\Program Files\\\\Git\\\\usr\\\\bin\\\\core_perl', 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.CPL', 'PGBIN': 'C:\\\\Program Files\\\\PostgreSQL\\\\14\\\\bin', 'PGDATA': 'C:\\\\Program Files\\\\PostgreSQL\\\\14\\\\data', 'PGPASSWORD': 'root', 'PGROOT': 'C:\\\\Program Files\\\\PostgreSQL\\\\14', 'PGUSER': 'postgres', 'PHPROOT': 'c:\\\\tools\\\\php', 'PIPELINE_REPOSITORY_NAME': 'explosion/cython-blis', 'PIPELINE_WORKSPACE': 'D:\\\\a\\\\1', 'PIPX_BIN_DIR': 'C:\\\\Program Files (x86)\\\\pipx_bin', 'PIPX_HOME': 'C:\\\\Program Files (x86)\\\\pipx', 'PKG_CONFIG_PATH': 'C:\\\\Program Files\\\\Git\\\\mingw64\\\\lib\\\\pkgconfig;C:\\\\Program Files\\\\Git\\\\mingw64\\\\share\\\\pkgconfig', 'PKG_CONFIG_SYSTEM_INCLUDE_PATH': 'C:/Program Files/Git/mingw64/include', 'PKG_CONFIG_SYSTEM_LIBRARY_PATH': 'C:/Program Files/Git/mingw64/lib', 'PLINK_PROTOCOL': 'ssh', 'POWERSHELL_DISTRIBUTION_CHANNEL': 'Azure-DevOps-win22', 'POWERSHELL_UPDATECHECK': 'Off', 'PROCESSOR_ARCHITECTURE': 'AMD64', 'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 79 Stepping 1, GenuineIntel', 'PROCESSOR_LEVEL': '6', 'PROCESSOR_REVISION': '4f01', 'PROGRAMFILES': 'C:\\\\Program Files', 'PROMPT': '$P$G', 'PSEXECUTIONPOLICYPREFERENCE': 'Unrestricted', 'PSMODULEANALYSISCACHEPATH': 'C:\\\\PSModuleAnalysisCachePath\\\\ModuleAnalysisCache', 'PSMODULEPATH': 'C:\\\\Users\\\\VssAdministrator\\\\Documents\\\\WindowsPowerShell\\\\Modules;C:\\\\\\\\Modules\\\\azurerm_2.1.0;C:\\\\\\\\Modules\\\\azure_2.1.0;C:\\\\Users\\\\packer\\\\Documents\\\\WindowsPowerShell\\\\Modules;C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\Windows\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules;C:\\\\Program Files\\\\Microsoft SQL Server\\\\130\\\\Tools\\\\PowerShell\\\\Modules\\\\', 'PUBLIC': 'C:\\\\Users\\\\Public', 'PWD': 'D:/a/1/s/flame-blis', 'PYTHON_VERSION': '3.9', 'PROGRAMDATA': 'C:\\\\ProgramData', 'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)', 'PROGRAMW6432': 'C:\\\\Program Files', 'RESOURCES_TRIGGERINGALIAS': '', 'RESOURCES_TRIGGERINGCATEGORY': '', 'RETIRE_AZURERM_POWERSHELL_MODULE': 'true', 'ROSETTA2_WARNING': 'true', 'RTOOLS44_HOME': 'C:\\\\rtools44', 'RUNNER_TOOLSDIRECTORY': 'C:\\\\hostedtoolcache\\\\windows', 'RUNNER_TOOL_CACHE': 'C:\\\\hostedtoolcache\\\\windows', 'SBT_HOME': 'C:\\\\Program Files (x86)\\\\sbt\\\\', 'SELENIUM_JAR_PATH': 'C:\\\\selenium\\\\selenium-server.jar', 'SHELL': 'C:\\\\Program Files\\\\Git\\\\usr\\\\bin\\\\bash.exe', 'SHLVL': '1', 'SSH_ASKPASS': 'C:/Program Files/Git/mingw64/bin/git-askpass.exe', 'STATS_BLT': 'true', 'STATS_D': 'true', 'STATS_D_D': 'true', 'STATS_D_TC': 'true', 'STATS_EXT': 'true', 'STATS_EXTP': 'https://provjobdprod.z13.web.core.windows.net/settings/provjobdsettings-latest/provjobd.data', 'STATS_PIP': 'false', 'STATS_RDCL': 'true', 'STATS_TRP': 'true', 'STATS_UE': 'true', 'STATS_V3PS': 'true', 'STATS_VMD': 'true', 'STATS_VMFE': 'true', 'SYSTEM': 'build', 'SYSTEMDRIVE': 'C:', 'SYSTEMROOT': 'C:\\\\Windows', 'SYSTEM_ARTIFACTSDIRECTORY': 'D:\\\\a\\\\1\\\\a', 'SYSTEM_COLLECTIONID': '116cc368-5c0c-4eb4-bb44-7f3fa5bdce14', 'SYSTEM_COLLECTIONURI': 'https://dev.azure.com/explosion-ai/', 'SYSTEM_CULTURE': 'en-US', 'SYSTEM_DEBUG': 'false', 'SYSTEM_DEFAULTWORKINGDIRECTORY': 'D:\\\\a\\\\1\\\\s', 'SYSTEM_DEFINITIONID': '6', 'SYSTEM_DEFINITIONNAME': 'explosion.cython-blis', 'SYSTEM_ENABLEACCESSTOKEN': 'SecretVariable', 'SYSTEM_HOSTTYPE': 'build', 'SYSTEM_ISAZUREVM': '1', 'SYSTEM_ISDOCKERCONTAINER': '0', 'SYSTEM_ISSCHEDULED': 'False', 'SYSTEM_JOBATTEMPT': '1', 'SYSTEM_JOBDISPLAYNAME': 'JSONL Python39Windows', 'SYSTEM_JOBID': 'b59b5e76-e04f-54cc-443e-737e71f5428d', 'SYSTEM_JOBIDENTIFIER': 'JSONL.Python39Windows', 'SYSTEM_JOBNAME': 'Python39Windows', 'SYSTEM_JOBPARALLELISMTAG': 'Public', 'SYSTEM_JOBPOSITIONINPHASE': '2', 'SYSTEM_JOBTIMEOUT': '60', 'SYSTEM_OIDCREQUESTURI': 'https://dev.azure.com/explosion-ai/5c6613e9-6ccf-48bd-81de-dbc3b0a6f957/_apis/distributedtask/hubs/build/plans/5471f6a2-5d2d-4ea1-bf2f-6682941b17de/jobs/b59b5e76-e04f-54cc-443e-737e71f5428d/oidctoken', 'SYSTEM_PARALLELEXECUTIONTYPE': 'MultiConfiguration', 'SYSTEM_PHASEATTEMPT': '1', 'SYSTEM_PHASEDISPLAYNAME': 'JSONL', 'SYSTEM_PHASEID': 'ecb95708-c2a5-5456-f379-96cd8090c2a6', 'SYSTEM_PHASENAME': 'JSONL', 'SYSTEM_PIPELINESTARTTIME': '2025-01-10 17:44:05+00:00', 'SYSTEM_PLANID': '5471f6a2-5d2d-4ea1-bf2f-6682941b17de', 'SYSTEM_POSTLINESSPEED': '10000', 'SYSTEM_PULLREQUEST_ISFORK': 'False', 'SYSTEM_SERVERTYPE': 'Hosted', 'SYSTEM_STAGEATTEMPT': '1', 'SYSTEM_STAGEDISPLAYNAME': '__default', 'SYSTEM_STAGEID': '96ac2280-8cb4-5df5-99de-dd2da759617d', 'SYSTEM_STAGENAME': '__default', 'SYSTEM_TASKDEFINITIONSURI': 'https://dev.azure.com/explosion-ai/', 'SYSTEM_TASKDISPLAYNAME': 'Generate JSONL (Windows)', 'SYSTEM_TASKINSTANCEID': 'd4997dda-70a7-5ff3-3edd-fa7524db0f8f', 'SYSTEM_TASKINSTANCENAME': 'CmdLine5', 'SYSTEM_TEAMFOUNDATIONCOLLECTIONURI': 'https://dev.azure.com/explosion-ai/', 'SYSTEM_TEAMFOUNDATIONSERVERURI': 'https://dev.azure.com/explosion-ai/', 'SYSTEM_TEAMPROJECT': 'Public', 'SYSTEM_TEAMPROJECTID': '5c6613e9-6ccf-48bd-81de-dbc3b0a6f957', 'SYSTEM_TIMELINEID': '5471f6a2-5d2d-4ea1-bf2f-6682941b17de', 'SYSTEM_TOTALJOBSINPHASE': '3', 'SYSTEM_WORKFOLDER': 'D:\\\\a', 'TASK_DISPLAYNAME': 'Generate JSONL (Windows)', 'TASK_PUBLISHTELEMETRY': 'True', 'TASK_SKIPTRANSLATORFORCHECKOUT': 'False', 'TEMP': 'C:\\\\Users\\\\VSSADM~1\\\\AppData\\\\Local\\\\Temp', 'TERM': 'xterm-256color', 'TF_BUILD': 'True', 'TMP': 'C:\\\\Users\\\\VSSADM~1\\\\AppData\\\\Local\\\\Temp', 'TMPDIR': 'C:\\\\Users\\\\VSSADM~1\\\\AppData\\\\Local\\\\Temp', 'USEPYTHONVERSION_PYTHONLOCATION': 'C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.9.13\\\\x64', 'USERDOMAIN': 'fv-az618-395', 'USERDOMAIN_ROAMINGPROFILE': 'fv-az618-395', 'USERNAME': 'VssAdministrator', 'USERPROFILE': 'C:\\\\Users\\\\VssAdministrator', 'USE_GIT_LONG_PATHS': 'true', 'USE_LATEST_GIT_VERSION': 'true', 'USE_MSAL': 'true', 'USE_MSDEPLOY_TOKEN_AUTH': 'true', 'VCPKG_INSTALLATION_ROOT': 'C:\\\\vcpkg', 'VSTSAGENT_CONTINUE_AFTER_CANCEL_PROCESSTREEKILL_ATTEMPT': 'true', 'VSTSAGENT_DOCKER_ACTION_RETRIES': 'true', 'VSTS_AGENT_PERFLOG': 'c:\\\\vsts\\\\perflog', 'VSTS_PROCESS_LOOKUP_ID': 'vsts_1a8accd9-c4cc-417c-b11d-eed7c6b9f10a', 'WINDIR': 'C:\\\\Windows', 'WIX': 'C:\\\\Program Files (x86)\\\\WiX Toolset v3.14\\\\', '_': 'C:/hostedtoolcache/windows/Python/3.9.13/x64/python', 'AGENT.JOBSTATUS': 'Succeeded', 'NPM_CONFIG_PREFIX': 'C:\\\\npm\\\\prefix'}\n",
      "      [COMMAND] C:\\Program Files\\LLVM\\bin\\clang.exe -c C:\\Users\\22601\\AppData\\Local\\Temp\\pip-install-bkc3ax37\\blis_9d221d6f68a143659196c90db7ee73be\\blis\\_src\\config\\bulldozer\\bli_cntx_init_bulldozer.c -o C:\\Users\\22601\\AppData\\Local\\Temp\\tmpc18wwv7g\\bli_cntx_init_bulldozer.o -O3 -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.7.0\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude\\windows-x86_64 -I.\\frame\\3\\ -I.\\frame\\ind\\ukernels\\ -I.\\frame\\3\\ -I.\\frame\\1m\\ -I.\\frame\\1f\\ -I.\\frame\\1\\ -I.\\frame\\include -IC:\\Users\\22601\\AppData\\Local\\Temp\\pip-install-bkc3ax37\\blis_9d221d6f68a143659196c90db7ee73be\\blis\\_src\\include\\windows-x86_64\n",
      "      error: [WinError 2] The system cannot find the file specified\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for blis\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (blis)\n",
      "c:\\Users\\22601\\anaconda3\\envs\\finer_env\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "# 📦 One‑time installs (comment out after first run)\n",
    "!pip install pdfminer.six spacy pandas -q\n",
    "!python -m spacy download en_core_web_lg -q\n",
    "# Windows 环境下推荐：\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install \"blis==0.7.11\" --only-binary :all:\n",
    "!pip install \"spacy==3.7.2\" --prefer-binary\n",
    "!python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab2789",
   "metadata": {},
   "source": [
    "> *We use ****pdfminer.six**** for text extraction and ****spaCy**** (**``**) for classic rule‑based NER.*\n",
    "\n",
    "---\n",
    "\n",
    "## 2  Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69bae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, sys, ast\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.pdfdocument import PDFSyntaxError\n",
    "\n",
    "# Load spaCy model once ↓\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345df42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3  PDF Utilities\n",
    "\n",
    "\n",
    "\n",
    "### 3.1 `extract_text_from_pdf`\n",
    "\n",
    "Extract all text while silencing pdfminer warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27165d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Return full text of a PDF or **None** on failure.\"\"\"\n",
    "    out = io.StringIO()\n",
    "    old_stderr = sys.stderr\n",
    "    sys.stderr = open(os.devnull, \"w\")\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            extract_text_to_fp(f, out)\n",
    "        return out.getvalue()\n",
    "    except PDFSyntaxError as e:\n",
    "        print(f\"[PDFSyntaxError] {pdf_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {pdf_path}: {e}\")\n",
    "    finally:\n",
    "        sys.stderr.close(); sys.stderr = old_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62651a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4  Entity Extraction\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 `extract_entities`\n",
    "\n",
    "Captures **PERSON**, **ORG**, quick‑n‑dirty **INDUSTRY** terms, and sentence‑level context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50148c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text: str) -> Dict[str, list]:\n",
    "    if not text:\n",
    "        return {k: [] for k in (\"PERSON\",\"ORG\",\"INDUSTRY\",\"ORG_CONTEXT\")}\n",
    "\n",
    "    doc = nlp(text)\n",
    "    ents = {\"PERSON\": [], \"ORG\": [], \"INDUSTRY\": [], \"ORG_CONTEXT\": []}\n",
    "\n",
    "    # Named entities\n",
    "    org_spans = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            ents[\"PERSON\"].append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            org_spans.append(ent); ents[\"ORG\"].append(ent.text)\n",
    "\n",
    "    # Simple dictionary lookup for industries (extend as needed)\n",
    "    industry_terms = [\n",
    "        \"banking\",\"finance\",\"technology\",\"real estate\",\n",
    "        \"telecommunications\",\"manufacturing\",\"healthcare\"\n",
    "    ]\n",
    "    ents[\"INDUSTRY\"] = [t for t in industry_terms if t in text.lower()]\n",
    "\n",
    "    # Sentence context for each organisation\n",
    "    for org in org_spans:\n",
    "        sent = next(s for s in doc.sents if org.start_char >= s.start_char <= org.end_char <= s.end_char)\n",
    "        ents[\"ORG_CONTEXT\"].append({'organization': org.text, 'context': sent.text})\n",
    "\n",
    "    # Deduplicate\n",
    "    for k in (\"PERSON\",\"ORG\",\"INDUSTRY\"):\n",
    "        ents[k] = list(set(ents[k]))\n",
    "    ents[\"ORG_CONTEXT\"] = [dict(t) for t in {tuple(d.items()) for d in ents[\"ORG_CONTEXT\"]}]\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f987a59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5  Relationship Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40402686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_org_industry(df_context: pd.DataFrame, industries: List[str]) -> List[Dict]:\n",
    "    rels = []\n",
    "    if df_context.empty or not industries:\n",
    "        return rels\n",
    "    for _, row in df_context.iterrows():\n",
    "        ctx = str(row['Context']).lower()\n",
    "        for ind in industries:\n",
    "            if ind.lower() in ctx:\n",
    "                rels.append({'Filename': row['Filename'],\n",
    "                             'Organization': row['Organization'],\n",
    "                             'Industry': ind})\n",
    "    return [dict(t) for t in {tuple(d.items()) for d in rels}]\n",
    "\n",
    "\n",
    "def infer_person_org(doc: spacy.tokens.Doc, persons: List[str], orgs: List[str]) -> List[Dict]:\n",
    "    rels = []\n",
    "    p_set, o_set = set(persons), set(orgs)\n",
    "    for sent in doc.sents:\n",
    "        found_p = [p for p in p_set if p in sent.text]\n",
    "        found_o = [o for o in o_set if o in sent.text]\n",
    "        for p in found_p:\n",
    "            for o in found_o:\n",
    "                rels.append({'Person': p, 'Organization': o})\n",
    "    return [dict(t) for t in {tuple(d.items()) for d in rels}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9bff6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6  Batch Processing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0c0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reports(pdf_dir: str, base: str = 'sgx') -> None:\n",
    "    \"\"\"Loop through PDFs, save four CSVs: *_entities, *_org_context,\n",
    "    *_person_org_relationships, *_org_industry_relationships.\"\"\"\n",
    "\n",
    "    data_ents, data_ctx, data_p2o = [], [], []\n",
    "    pdfs = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]\n",
    "    if not pdfs:\n",
    "        print('[!] No PDF found'); return\n",
    "\n",
    "    for f in pdfs:\n",
    "        print('→', f)\n",
    "        txt = extract_text_from_pdf(os.path.join(pdf_dir, f))\n",
    "        if not txt:\n",
    "            continue\n",
    "        doc = nlp(txt)\n",
    "        ent = extract_entities(txt)\n",
    "        # ---------------- Entities -----------------\n",
    "        data_ents.append({\n",
    "            'Filename': f,\n",
    "            'Persons': ent['PERSON'],\n",
    "            'Organizations': ent['ORG'],\n",
    "            'Industries': ent['INDUSTRY']\n",
    "        })\n",
    "        # ---------------- Org‑level context (capitalised keys!) -----------------\n",
    "        for c in ent['ORG_CONTEXT']:\n",
    "            data_ctx.append({\n",
    "                'Filename': f,\n",
    "                'Organization': c['organization'],\n",
    "                'Context': c['context']\n",
    "            })\n",
    "        # ---------------- Person ↔ Org relationships -----------------\n",
    "        data_p2o.extend([\n",
    "            {'Filename': f, **r} for r in infer_person_org(doc, ent['PERSON'], ent['ORG'])\n",
    "        ])\n",
    "\n",
    "    # --- Save intermediate CSVs ---\n",
    "    df_ent = pd.DataFrame(data_ents)\n",
    "    df_ent.to_csv(f'{base}_entities.csv', index=False)\n",
    "\n",
    "    df_ctx = pd.DataFrame(data_ctx)\n",
    "    df_ctx.to_csv(f'{base}_org_context.csv', index=False)\n",
    "\n",
    "    df_p2o = pd.DataFrame(data_p2o)\n",
    "    df_p2o.to_csv(f'{base}_person_org_relationships.csv', index=False)\n",
    "\n",
    "    # --- Org ↔ Industry ---\n",
    "    # Flatten unique industry terms\n",
    "    all_inds = sorted({i for sub in df_ent['Industries'] for i in (sub if isinstance(sub, list) else [sub])})\n",
    "    rel_oi = infer_org_industry(df_ctx, all_inds)\n",
    "    pd.DataFrame(rel_oi).to_csv(f'{base}_org_industry_relationships.csv', index=False)\n",
    "\n",
    "    print('✅  Processing complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad72e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7  Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be0b4887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 842974_E01238.pdf\n",
      "✅  Processing complete.\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = 'C:\\\\Users\\\\22601\\\\Downloads\\\\finer\\\\data'   # ← change as needed\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "process_reports(PDF_DIR, base='sgx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a9d7d",
   "metadata": {},
   "source": [
    "After execution you will find these files in the working directory:\n",
    "\n",
    "- `sgx_entities.csv`\n",
    "- `sgx_org_context.csv`\n",
    "- `sgx_person_org_relationships.csv`\n",
    "- `sgx_org_industry_relationships.csv`\n",
    "\n",
    "---\n",
    "\n",
    "## 8  Combine Outputs\n",
    "\n",
    "If you prefer a single edge‑list style CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c50d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_relationships(base='sgx'):\n",
    "    try:\n",
    "        df_oi = pd.read_csv(f'{base}_org_industry_relationships.csv')\n",
    "        df_p2o = pd.read_csv(f'{base}_person_org_relationships.csv')\n",
    "    except FileNotFoundError:\n",
    "        print('[!] Run `process_reports` first'); return\n",
    "\n",
    "    rows = []\n",
    "    rows += [{'Filename': r.Filename, 'Entity1': r.Organization, 'Relation': 'ASSOCIATED_INDUSTRY', 'Entity2': r.Industry}\n",
    "              for r in df_oi.itertuples(index=False)]\n",
    "    rows += [{'Filename': r.Filename, 'Entity1': r.Person, 'Relation': 'ASSOCIATED_WITH',    'Entity2': r.Organization}\n",
    "              for r in df_p2o.itertuples(index=False)]\n",
    "    pd.DataFrame(rows).to_csv(f'{base}_combined_relationships.csv', index=False)\n",
    "    print('🔗  Saved', f'{base}_combined_relationships.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd45a0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9  Next Steps / TODOs\n",
    "\n",
    "\n",
    "\n",
    "- **Improve industry detection**: replace keyword list with a trained classifier or Gazetteer.\n",
    "- **Add Chinese support** (`zh_core_web_lg`) for bilingual reports.\n",
    "- **Switch to GPU‑accelerated extraction** with **PyMuPDF** or **pdfplumber** for speed.\n",
    "- **Unit tests** for all helper functions.\n",
    "- **Packaging**: turn this notebook into a CLI (`python -m sgx_ner path/to/pdfs`).\n",
    "\n",
    "---\n",
    "\n",
    "> *Notebook prepared from the original `ner.py` script — organised, deduplicated, and documented for clarity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e1ec26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# URI examples: \"neo4j://localhost\", \"neo4j+s://xxx.databases.neo4j.io\"\n",
    "URI = \"neo4j+s://a0183311.databases.neo4j.io\"\n",
    "AUTH = (\"neo4j\", \"g9nN2A4Pp_ExSlJtescRkeZBI9BhhZulnwawbZla2oA\")\n",
    "\n",
    "with GraphDatabase.driver(URI\n",
    ", auth=AUTH\n",
    ") as driver:\n",
    "    driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39a15b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility: push an (entities, relations) payload into Neo4j/Aura.\n",
    "\n",
    "* Reads NEO4J_URI / NEO4J_USERNAME / NEO4J_PASSWORD from the process\n",
    "  (or a .env file dropped by Aura’s “Download Credentials” button).\n",
    "* Works with any secured Aura instance because it uses the `neo4j+s://`\n",
    "  scheme and leverages py2neo's built-in routing/TLS support (>=2021.2).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from dotenv import load_dotenv           # pip install python-dotenv\n",
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Public API\n",
    "# --------------------------------------------------------------------------- #\n",
    "def store_entities_relations_in_neo4j(\n",
    "    entities: Dict[str, Dict[str, str]],\n",
    "    relations: List[Tuple[str, str, str]],\n",
    "    *,\n",
    "    uri: str | None = None,\n",
    "    user: str | None = None,\n",
    "    password: str | None = None,\n",
    "    clear: bool = False,\n",
    ") -> Graph:\n",
    "    \"\"\"\n",
    "    Push entities & relations to Neo4j/Aura.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entities   : {\"entity_id\": {\"type\": \"Label\", **props}}\n",
    "    relations  : [(source_id, \"REL_TYPE\", target_id), ...]\n",
    "    uri        : Override for Neo4j URI.  Defaults to $NEO4J_URI.\n",
    "    user       : Override for username.    Defaults to $NEO4J_USERNAME.\n",
    "    password   : Override for password.    Defaults to $NEO4J_PASSWORD.\n",
    "    clear      : If True, wipes the DB with `MATCH (n) DETACH DELETE n`.\n",
    "    \"\"\"\n",
    "    _bootstrap_dotenv()\n",
    "\n",
    "    uri = uri or os.getenv(\"NEO4J_URI\", \"neo4j+s://a0183311.databases.neo4j.io\")\n",
    "    user = user or os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "    password = password or os.getenv(\"NEO4J_PASSWORD\", \"g9nN2A4Pp_ExSlJtescRkeZBI9BhhZulnwawbZla2oA\")\n",
    "\n",
    "    if not password:\n",
    "        raise ValueError(\n",
    "            \"Neo4j password not provided.  \"\n",
    "            \"Set NEO4J_PASSWORD in your environment or pass `password=`.\"\n",
    "        )\n",
    "\n",
    "    graph = Graph(uri, auth=(user, password))\n",
    "\n",
    "    if clear:\n",
    "        graph.run(\"MATCH (n) DETACH DELETE n\")   # beware of large TXNs!\n",
    "\n",
    "    # --- create entity nodes ------------------------------------------------ #\n",
    "    entity_nodes: Dict[str, Node] = {}\n",
    "    for eid, meta in entities.items():\n",
    "        label = meta.get(\"type\", \"Entity\")\n",
    "        props = {k: v for k, v in meta.items() if k != \"type\"}\n",
    "        props.setdefault(\"name\", eid)\n",
    "        node = Node(label, **props)\n",
    "        graph.merge(node, label, \"name\")         # idempotent insert/update\n",
    "        entity_nodes[eid] = node\n",
    "\n",
    "    # --- create relationships ---------------------------------------------- #\n",
    "    for src, rel_type, tgt in relations:\n",
    "        if src in entity_nodes and tgt in entity_nodes:\n",
    "            rel = Relationship(entity_nodes[src], rel_type, entity_nodes[tgt])\n",
    "            graph.merge(rel)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Helpers\n",
    "# --------------------------------------------------------------------------- #\n",
    "def _bootstrap_dotenv() -> None:\n",
    "    \"\"\"\n",
    "    Load a `.env` file from the current working directory if it exists.\n",
    "    This is where Aura Free saves the URI / user / password bundle.\n",
    "    In a Jupyter notebook, this is typically the directory of the .ipynb file.\n",
    "    \"\"\"\n",
    "    # load_dotenv() will automatically search for a .env file in the current\n",
    "    # directory and its parents. This is compatible with Jupyter notebooks.\n",
    "    # It will not override existing environment variables.\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5122f7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2731 relationships from CSV.\n",
      "Prepared 717 unique entities and 2731 relations to be stored.\n",
      "\n",
      "Successfully stored data in Neo4j.\n",
      "Graph details: Graph('neo4j+s://a0183311.databases.neo4j.io:7687')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define the path to your data file\n",
    "#    Make sure this path is correct for your environment.\n",
    "csv_file_path = 'g:/My Drive/NUS MSBA SEM2/UOB/SGX Annual Reports/sgx_person_org_relationships.csv'\n",
    "\n",
    "# 2. Load the relations data from your CSV\n",
    "try:\n",
    "    df_relations = pd.read_csv(csv_file_path)\n",
    "    # Ensure we don't have rows with missing Person or Organization\n",
    "    df_relations.dropna(subset=['Person', 'Organization'], inplace=True)\n",
    "    print(f\"Successfully loaded {len(df_relations)} relationships from CSV.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_file_path}\")\n",
    "    # Create an empty DataFrame to prevent further errors if file is not found\n",
    "    df_relations = pd.DataFrame(columns=['Person', 'Organization'])\n",
    "\n",
    "# 3. Prepare the 'entities' and 'relations' data structures for Neo4j\n",
    "entities_to_store = {}\n",
    "relations_to_store = []\n",
    "\n",
    "if not df_relations.empty:\n",
    "    # Create entity entries for each unique person and organization\n",
    "    for person in df_relations['Person'].unique():\n",
    "        entities_to_store[str(person)] = {'type': 'Person'}\n",
    "    \n",
    "    for org in df_relations['Organization'].unique():\n",
    "        entities_to_store[str(org)] = {'type': 'Organization'}\n",
    "\n",
    "    # Create relation entries from each row in the DataFrame\n",
    "    for _, row in df_relations.iterrows():\n",
    "        source_person = str(row['Person'])\n",
    "        target_org = str(row['Organization'])\n",
    "        # You can customize the relationship type if needed\n",
    "        relation_type = \"ASSOCIATED_WITH\" \n",
    "        relations_to_store.append((source_person, relation_type, target_org))\n",
    "\n",
    "    print(f\"Prepared {len(entities_to_store)} unique entities and {len(relations_to_store)} relations to be stored.\")\n",
    "\n",
    "    # 4. Call the function to store the data in your Neo4j database\n",
    "    # This will use the credentials from your .env file or the defaults in the function.\n",
    "    # The `clear=True` flag will wipe the database before adding new data.\n",
    "    # Set clear=False if you want to add to existing data without deleting it first.\n",
    "    try:\n",
    "        graph = store_entities_relations_in_neo4j(\n",
    "            entities=entities_to_store,\n",
    "            relations=relations_to_store,\n",
    "            clear=True\n",
    "        )\n",
    "        print(\"\\nSuccessfully stored data in Neo4j.\")\n",
    "        print(f\"Graph details: {graph}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while connecting to or writing to Neo4j: {e}\")\n",
    "else:\n",
    "    print(\"No data to store. Please check the CSV file path and its content.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e2158",
   "metadata": {},
   "source": [
    "### unusable because a lot of meaningless entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457af99",
   "metadata": {},
   "source": [
    "# using gemini 2.5 pro to perform NER again on the same document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f241e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIzaSyDfwYYn4mgi1HE2EbOq-QiLE_sRvo0XknI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28562347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "sgx_ner_to_neo4j.py\n",
    "-------------------\n",
    "\n",
    "End‑to‑end pipeline to extract People‑↔︎Organisation role relations from an SGX\n",
    "annual‑report PDF and push them into Neo4j Aura using Google Gemini 2.5 Pro for\n",
    "NER.\n",
    "\n",
    "⚙️  Requirements\n",
    "    pip install google-generativeai pdfplumber python-dotenv langchain py2neo\n",
    "\n",
    "The script expects these **environment variables** (e.g. in a `.env` file):\n",
    "\n",
    "    GOOGLE_API_KEY      # your Google AI Developer key\n",
    "    NEO4J_URI           # e.g. neo4j+s://a0183311.databases.neo4j.io\n",
    "    NEO4J_USERNAME      # neo4j\n",
    "    NEO4J_PASSWORD      # 40‑char secret from Aura\n",
    "    PDF_PATH            # path to local annual‑report PDF\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "import pdfplumber                         # PDF text extraction\n",
    "from dotenv import load_dotenv            # env helper\n",
    "from google import genai                     # <-- main change\n",
    "import google.generativeai as genai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from py2neo import Graph, Node, Relationship\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9538037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# ----------------------------  CONFIGURATION  ------------------------------ #\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "load_dotenv()                             # loads .env if present\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyDfwYYn4mgi1HE2EbOq-QiLE_sRvo0XknI\")\n",
    "PDF_PATH       = os.getenv(\"PDF_PATH\", \"C:\\\\Users\\\\22601\\\\Downloads\\\\finer\\\\data\\\\842974_E01238.pdf\")\n",
    "CHUNK_SIZE     = int(os.getenv(\"CHUNK_SIZE\", 3000))\n",
    "CHUNK_OVERLAP  = int(os.getenv(\"CHUNK_OVERLAP\", 250))\n",
    "MODEL_NAME     = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8847d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new instance from my edu account\n",
    "NEO4J_URI      = os.getenv(\"NEO4J_URI\", \"neo4j+s://d8d4e86b.databases.neo4j.io\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"IVVi_p1Rl2ca-O5g5ULkd5KHtg2uSXkLaj1So_oHL4Q\")\n",
    "NEO4J_CLEAR    = os.getenv(\"NEO4J_CLEAR\", \"false\").lower() == \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ee6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required environment variables\n",
    "for var_name in (\"GOOGLE_API_KEY\", \"NEO4J_URI\", \"NEO4J_USERNAME\", \"NEO4J_PASSWORD\"):\n",
    "    if not locals()[var_name]:\n",
    "        raise EnvironmentError(f\"Missing required environment variable: {var_name}\")\n",
    "\n",
    "# Model and Chunking Configuration\n",
    "MODEL_NAME = \"gemini-2.5-pro\" # Using a modern model name\n",
    "CHUNK_SIZE = 8000\n",
    "CHUNK_OVERLAP = 400\n",
    "NEO4J_CLEAR = False # Set to False to append to existing graph data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97356d8",
   "metadata": {},
   "source": [
    "# old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # --------------------------------------------------------------------------- #\n",
    "# # ---------------------- GEMINI CLIENT INITIALIZATION ----------------------- #\n",
    "# # --------------------------------------------------------------------------- #\n",
    "\n",
    "# # Configure the GenAI client with the API key\n",
    "# genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# # Create the Generative Model instance with the system prompt\n",
    "# gemini_ner_model = genai.GenerativeModel(\n",
    "#     model_name=MODEL_NAME,\n",
    "#     system_instruction=SYSTEM_PROMPT\n",
    "# )\n",
    "\n",
    "\n",
    "# # --------------------------------------------------------------------------- #\n",
    "# # ---------------------------  PDF HELPERS  --------------------------------- #\n",
    "# # --------------------------------------------------------------------------- #\n",
    "\n",
    "# def extract_text_from_pdf(path: str) -> str:\n",
    "#     \"\"\"Return concatenated text from every page of a PDF.\"\"\"\n",
    "#     text_parts = []\n",
    "#     with pdfplumber.open(path) as pdf:\n",
    "#         for page in pdf.pages:\n",
    "#             page_text = page.extract_text() or \"\"\n",
    "#             text_parts.append(page_text)\n",
    "#     return \"\\n\".join(text_parts)\n",
    "\n",
    "\n",
    "# def chunk_text(text: str,\n",
    "#                chunk_size: int = CHUNK_SIZE,\n",
    "#                overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "#     \"\"\"Splits text into manageable chunks for the model.\"\"\"\n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=overlap,\n",
    "#         separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "#     )\n",
    "#     return splitter.split_text(text)\n",
    "\n",
    "# # --------------------------------------------------------------------------- #\n",
    "# # --------------------------  GEMINI NER  ----------------------------------- #\n",
    "# # --------------------------------------------------------------------------- #\n",
    "\n",
    "# def ner_chunk(chunk: str) -> List[Dict[str, str]]:\n",
    "#     \"\"\"\n",
    "#     Sends a text chunk to the Gemini model for NER and parses the response.\n",
    "#     This function is now fixed to use the current API.\n",
    "#     \"\"\"\n",
    "#     # Call the modern API on the initialized model object\n",
    "#     response = gemini_ner_model.generate_content(chunk)\n",
    "\n",
    "#     # Model returns a blob of lines; filter and parse JSON\n",
    "#     relations = []\n",
    "#     for line in response.text.strip().splitlines():\n",
    "#         line = line.strip()\n",
    "#         if not line:\n",
    "#             continue\n",
    "#         try:\n",
    "#             # First attempt: load the line as a clean JSON object\n",
    "#             obj = json.loads(line)\n",
    "#             if obj:  # Ensure it's not an empty object {}\n",
    "#                 relations.append(obj)\n",
    "#         except json.JSONDecodeError:\n",
    "#             # Second attempt: salvage with a greedy regex if model adds extra text\n",
    "#             match = re.search(r\"{.*}\", line)\n",
    "#             if match:\n",
    "#                 try:\n",
    "#                     relations.append(json.loads(match.group(0)))\n",
    "#                 except json.JSONDecodeError:\n",
    "#                     # Ignore lines that are truly malformed\n",
    "#                     pass\n",
    "#     return relations\n",
    "\n",
    "# # --------------------------------------------------------------------------- #\n",
    "# # ------------------------  NEO4J LOADER  ----------------------------------- #\n",
    "# # --------------------------------------------------------------------------- #\n",
    "\n",
    "# def push_to_neo4j(relations: List[Dict[str, str]]) -> None:\n",
    "#     \"\"\"Pushes the extracted person-role-company relations into a Neo4j graph.\"\"\"\n",
    "#     graph = Graph(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "#     if NEO4J_CLEAR:\n",
    "#         print(\"Clearing existing Neo4j database...\")\n",
    "#         graph.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "#     # Use a cache to avoid merging the same node multiple times\n",
    "#     node_cache = {}  # (Label, name) -> Node\n",
    "\n",
    "#     print(f\"Pushing {len(relations)} relations to Neo4j...\")\n",
    "#     for rel in relations:\n",
    "#         person = rel.get(\"person\")\n",
    "#         role = rel.get(\"role\")\n",
    "#         company = rel.get(\"company\")\n",
    "\n",
    "#         # Skip if any of the core components are missing\n",
    "#         if not all((person, role, company)):\n",
    "#             continue\n",
    "\n",
    "#         # Create or merge Person node\n",
    "#         key_person = (\"Person\", person)\n",
    "#         if key_person not in node_cache:\n",
    "#             node = Node(\"Person\", name=person)\n",
    "#             graph.merge(node, \"Person\", \"name\")\n",
    "#             node_cache[key_person] = node\n",
    "\n",
    "#         # Create or merge Company node\n",
    "#         key_comp = (\"Company\", company)\n",
    "#         if key_comp not in node_cache:\n",
    "#             node = Node(\"Company\", name=company)\n",
    "#             graph.merge(node, \"Company\", \"name\")\n",
    "#             node_cache[key_comp] = node\n",
    "\n",
    "#         # Create the relationship between the two nodes\n",
    "#         rel_obj = Relationship(\n",
    "#             node_cache[key_person],\n",
    "#             \"HAS_ROLE_AT\",\n",
    "#             node_cache[key_comp],\n",
    "#             role=role.lower() # Standardize role to lowercase\n",
    "#         )\n",
    "#         graph.merge(rel_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f55459",
   "metadata": {},
   "source": [
    "# new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c208bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOC_SYSTEM_PROMPT = \"\"\"You are an expert document analysis AI. Your task is to analyze the provided text, which represents the Table of Contents from a corporate annual report, and find the starting page number for the section detailing the company's directors.\n",
    "\n",
    "**Analysis Rules:**\n",
    "1.  Look for section titles like \"Board of Directors\", \"Directors' Profile\", \"Information on Directors\", or \"Corporate Governance\".\n",
    "2.  Extract the exact title and its corresponding starting page number.\n",
    "3.  The page number is typically the last number on the line associated with the title.\n",
    "\n",
    "**Output Format:**\n",
    "You MUST return ONLY a single JSON object with two keys:\n",
    "- `section_title`: The exact string of the section title you found.\n",
    "- `start_page`: The integer page number for that section.\n",
    "\n",
    "If you cannot confidently identify the section, return `null` for both values.\n",
    "\n",
    "---\n",
    "**Example:**\n",
    "\n",
    "**Input Text:**\n",
    "\"Message to Shareholders ..................... 2\n",
    "Financial Highlights ......................... 4\n",
    "Board of Directors ........................... 8\n",
    "Statement on Corporate Governance .......... 20\n",
    "Report of the Audit Committee .............. 35\"\n",
    "\n",
    "**Output JSON:**\n",
    "```json\n",
    "{\n",
    "  \"section_title\": \"Board of Directors\",\n",
    "  \"start_page\": 8\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cee2c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_SYSTEM_PROMPT = \"\"\"You are an expert-level financial-domain Named Entity Recognition (NER) and Relationship Extraction engine. Your task is to analyze text chunks extracted exclusively from the 'Board of Directors' or 'Directors' Profile' section of a company's annual report. Your goal is to create a structured JSON object containing all people, companies, and their relationships.\n",
    "\n",
    "**Extraction Rules:**\n",
    "1.  **Be Specific:** Do NOT extract generic, non-specific entities. Ignore terms like 'the company', 'the group', 'the university', 'our auditors', 'the school', 'a bank' unless they are part of a full proper noun.\n",
    "2.  **Differentiate Entities:** Carefully distinguish between a person's name and an organization's name. A person's name usually consists of a first and last name. A company name often includes suffixes like 'Ltd', 'Group', 'Holdings', or 'Corporation'.\n",
    "3.  **Canonicalize Names:** For each unique entity, determine its most complete, official name from the text to use as its `canonicalName`. All other references (e.g., acronyms, shorter names) should be listed in the `mentions` array.\n",
    "4.  **Extract Timestamps:** Only populate the `effectiveDate` field if a specific date or year is mentioned in direct connection to a role or appointment (e.g., \"appointed on 1 Jan 2024\", \"since 2022\"). If no date is present, the value must be `null`.\n",
    "\n",
    "**Output Format:**\n",
    "You MUST return ONLY a single JSON object with two keys: \"entities\" and \"relationships\".\n",
    "\n",
    "---\n",
    "**Example:**\n",
    "\n",
    "**Input Text:**\n",
    "\"Mr. Tan Ah Kow joined the board of directors of SGX Group in 2023. He has been the Chief Executive Officer of DBS Group Holdings Ltd. (also known as DBS) since his appointment on Feb 1, 2022. His colleague, Ms. Jane Lim, is a director at Keppel Ltd.\"\n",
    "\n",
    "**Output JSON:**\n",
    "```json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"entityId\": \"PERSON_1\", \"type\": \"Person\", \"canonicalName\": \"Tan Ah Kow\",\n",
    "      \"mentions\": [\"Mr. Tan Ah Kow\"]\n",
    "    },\n",
    "    {\n",
    "      \"entityId\": \"COMPANY_1\", \"type\": \"Company\", \"canonicalName\": \"SGX Group\",\n",
    "      \"mentions\": [\"SGX Group\"]\n",
    "    },\n",
    "    {\n",
    "      \"entityId\": \"COMPANY_2\", \"type\": \"Company\", \"canonicalName\": \"DBS Group Holdings Ltd.\",\n",
    "      \"mentions\": [\"DBS Group Holdings Ltd.\", \"DBS\"]\n",
    "    },\n",
    "    {\n",
    "      \"entityId\": \"PERSON_2\", \"type\": \"Person\", \"canonicalName\": \"Jane Lim\",\n",
    "      \"mentions\": [\"Ms. Jane Lim\"]\n",
    "    },\n",
    "    {\n",
    "      \"entityId\": \"COMPANY_3\", \"type\": \"Company\", \"canonicalName\": \"Keppel Ltd\",\n",
    "      \"mentions\": [\"Keppel Ltd\"]\n",
    "    }\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {\n",
    "      \"sourceEntityId\": \"PERSON_1\", \"targetEntityId\": \"COMPANY_1\",\n",
    "      \"role\": \"director\", \"effectiveDate\": \"2023\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceEntityId\": \"PERSON_1\", \"targetEntityId\": \"COMPANY_2\",\n",
    "      \"role\": \"chief executive officer\", \"effectiveDate\": \"2022-02-01\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceEntityId\": \"PERSON_2\", \"targetEntityId\": \"COMPANY_3\",\n",
    "      \"role\": \"director\", \"effectiveDate\": null\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f65fd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Dependency Imports\n",
    "import google.generativeai as genai\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "# --- Assume environment variables and constants are defined above ---\n",
    "# --- This includes the two new prompts: TOC_SYSTEM_PROMPT and NER_SYSTEM_PROMPT ---\n",
    "\n",
    "TOC_PAGE_LIMIT = 10 # How many pages to scan for the Table of Contents\n",
    "\n",
    "DIRECTOR_SECTION_END_KEYWORDS = [\n",
    "    \"directors' statement\", \"statement by directors\", \"independent auditor's report\",\n",
    "    \"financial statements\", \"remuneration report\"\n",
    "]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# ---------------------- GEMINI CLIENT INITIALIZATION ----------------------- #\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Model for finding the section in the Table of Contents\n",
    "gemini_toc_model = genai.GenerativeModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    system_instruction=TOC_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "# Model for performing the actual NER on the section text\n",
    "gemini_ner_model = genai.GenerativeModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    system_instruction=NER_SYSTEM_PROMPT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cb951eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# ---------------------- PDF HELPERS (MODIFIED) ----------------------------- #\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "def find_directors_section_page(toc_text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Uses an LLM to find the Board of Directors section page from ToC text.\"\"\"\n",
    "    try:\n",
    "        response = gemini_toc_model.generate_content(toc_text)\n",
    "        return json.loads(response.text)\n",
    "    except (json.JSONDecodeError, AttributeError, ValueError):\n",
    "        return None\n",
    "\n",
    "def extract_director_section_text(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Implements the two-step process:\n",
    "    1. Use LLM to find the director section page from the Table of Contents.\n",
    "    2. Extract text starting from that page until an end keyword is found.\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        # Step 1: Analyze the Table of Contents using the LLM\n",
    "        print(\"   → 1a. Analyzing Table of Contents with LLM...\")\n",
    "        toc_pages_text = \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages[:TOC_PAGE_LIMIT]])\n",
    "        section_info = find_directors_section_page(toc_pages_text)\n",
    "        \n",
    "        start_page = section_info.get(\"start_page\") if section_info else None\n",
    "        \n",
    "        # Step 2: Extract text based on the found page number\n",
    "        if start_page and isinstance(start_page, int):\n",
    "            print(f\"   → 1b. Section '{section_info.get('section_title')}' found. Starting extraction from page {start_page}.\")\n",
    "            # PDF pages are 0-indexed, but page numbers in reports are 1-indexed.\n",
    "            for page_num in range(start_page - 1, len(pdf.pages)):\n",
    "                page = pdf.pages[page_num]\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                lower_text = page_text.lower()\n",
    "                \n",
    "                # Stop if we hit a common section that follows the directors' report\n",
    "                if any(keyword in lower_text for keyword in DIRECTOR_SECTION_END_KEYWORDS):\n",
    "                    break\n",
    "                text_parts.append(page_text)\n",
    "        else:\n",
    "            print(\"[Warning] Could not determine section from ToC. Falling back to keyword search across document.\")\n",
    "            # Fallback logic from previous version\n",
    "            in_section = False\n",
    "            start_keywords = [\"board of directors\", \"directors' profile\"]\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                lower_text = page_text.lower()\n",
    "                if in_section and any(keyword in lower_text for keyword in DIRECTOR_SECTION_END_KEYWORDS):\n",
    "                    break\n",
    "                if not in_section and any(keyword in lower_text for keyword in start_keywords):\n",
    "                    in_section = True\n",
    "                if in_section:\n",
    "                    text_parts.append(page_text)\n",
    "\n",
    "    if not text_parts:\n",
    "         raise ValueError(\"Could not find the directors' section or extract any text from the PDF.\")\n",
    "\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"Splits text into manageable chunks.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=overlap, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# ----------- UNCHANGED FUNCTIONS (ner_chunk, push_to_neo4j) ---------------- #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def ner_chunk(chunk: str) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    # This function remains the same as the previous version\n",
    "    empty_response = {\"entities\": [], \"relationships\": []}\n",
    "    try:\n",
    "        response = gemini_ner_model.generate_content(chunk)\n",
    "        response_text = response.text.strip()\n",
    "        match = re.search(r\"```json\\s*({.*})\\s*```\", response_text, re.DOTALL)\n",
    "        if match: json_str = match.group(1)\n",
    "        else: json_str = response_text\n",
    "        return json.loads(json_str)\n",
    "    except Exception: return empty_response\n",
    "\n",
    "def push_to_neo4j(relations: List[Dict[str, Any]]) -> None:\n",
    "    # This function remains the same as the previous version\n",
    "    graph = Graph(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "    if NEO4J_CLEAR: graph.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    node_cache = {}\n",
    "    for rel in relations:\n",
    "        p_name, c_name, role, date = rel.get(\"person\"), rel.get(\"company\"), rel.get(\"role\"), rel.get(\"effectiveDate\")\n",
    "        if not all((p_name, c_name, role)): continue\n",
    "        k_person, k_comp = (\"Person\", p_name), (\"Company\", c_name)\n",
    "        if k_person not in node_cache:\n",
    "            node = Node(\"Person\", name=p_name); graph.merge(node, \"Person\", \"name\"); node_cache[k_person] = node\n",
    "        if k_comp not in node_cache:\n",
    "            node = Node(\"Company\", name=c_name); graph.merge(node, \"Company\", \"name\"); node_cache[k_comp] = node\n",
    "        rel_obj = Relationship(node_cache[k_person], \"HAS_ROLE_AT\", node_cache[k_comp], role=role.lower(), effective_date=date)\n",
    "        graph.merge(rel_obj)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# --------------------------- MAIN EXECUTION -------------------------------- #\n",
    "# --------------------------------------------------------------------------- #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b091bdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Starting PDF processing for: C:\\Users\\22601\\Downloads\\finer\\data\\842974_E01238.pdf\n",
      "   → 1a. Analyzing Table of Contents with LLM...\n",
      "[Warning] Could not determine section from ToC. Falling back to keyword search across document.\n",
      "✂️  Splitting extracted text into chunks...\n",
      "   → 6 chunks to process\n",
      "🧠 Processing chunks with Gemini NER...\n",
      "   Processing chunk 6/6...\n",
      "🔗 Extracted 197 total relations from 127 unique entities after merging.\n",
      "💾 Pushing data to Neo4j...\n",
      "\n",
      "Pipeline finished successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- Assume all previous code, including prompts, functions, and Gemini client initialization, is defined above ---\n",
    "# --- No changes are needed to the prompts or the functions themselves. ---\n",
    "print(\"📖 Starting PDF processing for:\", PDF_PATH)\n",
    "# The extract_director_section_text function handles finding the correct text\n",
    "raw_text = extract_director_section_text(PDF_PATH)\n",
    "\n",
    "print(\"✂️  Splitting extracted text into chunks...\")\n",
    "chunks = chunk_text(raw_text)\n",
    "print(f\"   → {len(chunks)} chunks to process\")\n",
    "\n",
    "# Master dictionary to hold de-duplicated entities, keyed by their NORMALIZED name\n",
    "master_entities = {}  # Key: normalized_name, Value: full entity object\n",
    "resolved_relations = []\n",
    "\n",
    "print(\"🧠 Processing chunks with Gemini NER...\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"   Processing chunk {i}/{len(chunks)}...\", end=\"\\r\")\n",
    "    \n",
    "    data = ner_chunk(chunk)\n",
    "    if not data.get(\"entities\"):\n",
    "        continue\n",
    "\n",
    "    # A map to resolve a chunk's temporary ID to a permanent, normalized name\n",
    "    chunk_id_to_normalized_name_map = {}\n",
    "    \n",
    "    # === MODIFICATION START: De-duplicate entities using a normalized key ===\n",
    "    for entity in data.get(\"entities\", []):\n",
    "        canonical_name = entity[\"canonicalName\"]\n",
    "        \n",
    "        # THE FIX: Create a normalized key for consistent lookup\n",
    "        normalized_name = canonical_name.lower().strip()\n",
    "        \n",
    "        # Map the temporary ID of this chunk to the permanent normalized name\n",
    "        chunk_id_to_normalized_name_map[entity[\"entityId\"]] = normalized_name\n",
    "\n",
    "        # If we have not seen this entity before (based on its normalized name), add it\n",
    "        if normalized_name not in master_entities:\n",
    "            master_entities[normalized_name] = entity\n",
    "        else:\n",
    "            # If we have seen this entity, merge the mentions to enrich the data\n",
    "            # This handles cases where \"DBS\" and \"DBS Group\" are found in different chunks\n",
    "            # but have the same canonical name (\"DBS Group Holdings Ltd.\")\n",
    "            existing_mentions = set(master_entities[normalized_name].get(\"mentions\", []))\n",
    "            new_mentions = set(entity.get(\"mentions\", []))\n",
    "            \n",
    "            # Also add the new canonicalName variation itself to the mentions list\n",
    "            existing_mentions.add(master_entities[normalized_name][\"canonicalName\"])\n",
    "            new_mentions.add(canonical_name)\n",
    "            \n",
    "            master_entities[normalized_name][\"mentions\"] = sorted(list(existing_mentions.union(new_mentions)))\n",
    "\n",
    "    # Resolve relationships using the map of normalized names\n",
    "    for rel in data.get(\"relationships\", []):\n",
    "        person_id = rel.get(\"sourceEntityId\")\n",
    "        company_id = rel.get(\"targetEntityId\")\n",
    "        \n",
    "        person_normalized_name = chunk_id_to_normalized_name_map.get(person_id)\n",
    "        company_normalized_name = chunk_id_to_normalized_name_map.get(company_id)\n",
    "        \n",
    "        if person_normalized_name and company_normalized_name:\n",
    "            # Crucially, retrieve the original CANONICAL name from the master list\n",
    "            # to use in the final relationship record.\n",
    "            person_canonical_name = master_entities[person_normalized_name]['canonicalName']\n",
    "            company_canonical_name = master_entities[company_normalized_name]['canonicalName']\n",
    "            \n",
    "            resolved_relations.append({\n",
    "                \"person\": person_canonical_name,\n",
    "                \"company\": company_canonical_name,\n",
    "                \"role\": rel.get(\"role\"),\n",
    "                \"effectiveDate\": rel.get(\"effectiveDate\")\n",
    "            })\n",
    "    # === MODIFICATION END ===\n",
    "\n",
    "print(f\"\\n🔗 Extracted {len(resolved_relations)} total relations from {len(master_entities)} unique entities after merging.\")\n",
    "\n",
    "if resolved_relations:\n",
    "    print(\"💾 Pushing data to Neo4j...\")\n",
    "    push_to_neo4j(resolved_relations)\n",
    "    print(\"\\nPipeline finished successfully!\")\n",
    "else:\n",
    "    print(\"\\nNo relations were extracted to push to the database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987ed47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finer_env",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
